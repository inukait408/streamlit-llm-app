from dotenv import load_dotenv

load_dotenv()
import streamlit as st
from langchain_openai import ChatOpenAI
from langchain.schema import SystemMessage, HumanMessage

# ã‚¿ã‚¤ãƒˆãƒ«ã¨ã‚¢ãƒ—ãƒªã®æ¦‚è¦
st.set_page_config(page_title="LLMå°‚é–€å®¶ç›¸è«‡ã‚¢ãƒ—ãƒª", layout="centered")
st.title("ğŸ§  LLMå°‚é–€å®¶ç›¸è«‡ã‚¢ãƒ—ãƒª")
st.write("""
ã“ã®ã‚¢ãƒ—ãƒªã§ã¯ã€é¸æŠã—ãŸå°‚é–€åˆ†é‡ã®AIå°‚é–€å®¶ã«è³ªå•ã§ãã¾ã™ã€‚  
ä»¥ä¸‹ã®ã‚¹ãƒ†ãƒƒãƒ—ã§ã”åˆ©ç”¨ãã ã•ã„ã€‚

1. å°‚é–€å®¶ã®ç¨®é¡ã‚’é¸æŠã—ã¦ãã ã•ã„  
2. è³ªå•ã‚’å…¥åŠ›ã—ã¦é€ä¿¡ã—ã¦ãã ã•ã„  
3. å›ç­”ãŒç”»é¢ä¸‹éƒ¨ã«è¡¨ç¤ºã•ã‚Œã¾ã™
""")

# å°‚é–€å®¶ã®é¸æŠ
expert_type = st.radio(
    "ç›¸è«‡ã—ãŸã„å°‚é–€å®¶ã®ã‚¿ã‚¤ãƒ—ã‚’é¸ã‚“ã§ãã ã•ã„ï¼š",
    ("ã‚­ãƒ£ãƒªã‚¢ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆ", "è‚²å…ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼", "å¥åº·ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼")
)

# ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
user_input = st.text_input("ã‚ãªãŸã®è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:")

# æ¸©åº¦è¨­å®š
temperature = st.slider("æ¸©åº¦ã‚’è¨­å®š:", 0.0, 1.0, 0.5)

# LLMå‘¼ã³å‡ºã—é–¢æ•°
def get_response_from_llm(input_text: str, expert_role: str) -> str:
    expert_prompts = {
        "ã‚­ãƒ£ãƒªã‚¢ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆ": "ã‚ãªãŸã¯çµŒé¨“è±Šå¯Œãªã‚­ãƒ£ãƒªã‚¢ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆã§ã™ã€‚è»¢è·ã‚„è·å ´ã§ã®æ‚©ã¿ã«å¯¾ã—ã¦ã€ä¸å¯§ã‹ã¤å…·ä½“çš„ã«ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã—ã¦ãã ã•ã„ã€‚",
        "è‚²å…ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼": "ã‚ãªãŸã¯ä¿è‚²å£«è³‡æ ¼ã‚’æŒã¤è‚²å…ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ã§ã™ã€‚å­è‚²ã¦ã«é–¢ã™ã‚‹æ‚©ã¿ã«å¯¾ã—ã¦ã€ã‚„ã•ã—ãå…·ä½“çš„ã«ç­”ãˆã¦ãã ã•ã„ã€‚",
        "å¥åº·ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼": "ã‚ãªãŸã¯æ „é¤Šå­¦ã¨é‹å‹•ç”Ÿç†å­¦ã®å°‚é–€å®¶ã§ã™ã€‚å¥åº·ã‚„ç”Ÿæ´»ç¿’æ…£ã«é–¢ã™ã‚‹ç›¸è«‡ã«å¯¾ã—ã¦ã€ç§‘å­¦çš„æ ¹æ‹ ã«åŸºã¥ã„ãŸã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’ã—ã¦ãã ã•ã„ã€‚"
    }

    llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=temperature)

    try:
        messages = [
            SystemMessage(content=expert_prompts[expert_role]),
            HumanMessage(content=input_text)
        ]
        response = llm(messages)
        return response.content
    except Exception as e:
        return f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"

# å…¥åŠ›ãŒã‚ã‚‹å ´åˆã«å‡¦ç†ã‚’å®Ÿè¡Œ
if user_input:
    if not user_input.strip():
        st.warning("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    else:
        with st.spinner("AIãŒè€ƒãˆã¦ã„ã¾ã™..."):
            result = get_response_from_llm(user_input, expert_type)
        st.success("AIã®å›ç­”:")
        st.write(result)